{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56a16226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import MNIST\n",
    "import idx2numpy\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd051f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN(nn.Module):\n",
    "    def __init__(self, in_shape, out_shape):\n",
    "        super().__init__()\n",
    "        fc1 = nn.Linear(in_shape, 64)\n",
    "        a1  = nn.ReLU()\n",
    "        d1  = nn.Dropout(p=0.5)\n",
    "        fc2 = nn.Linear(64, 32)\n",
    "        a2  = nn.ReLU()\n",
    "        d2  = nn.Dropout(p=0.4)\n",
    "        fc3 = nn.Linear(32, out_shape)\n",
    "        \n",
    "        # not applying log_softmax here, as it is applied later in \n",
    "        # the torch CCE loss\n",
    "        \n",
    "        self.nn = nn.Sequential(fc1, a1, d1, fc2, a2, d2, fc3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.nn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f4f3a81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/.local/lib/python3.9/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:174.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# define dataset splits \n",
    "transforms = T.Compose([T.ToTensor(), T.Normalize((0.5,),(0.5)), T.Lambda(lambda x: torch.flatten(x))])\n",
    "\n",
    "dataset_train = MNIST(root='/data/', transform=transforms, train=True)\n",
    "dataset_test  = MNIST(root='/data/', transform=transforms, train=False)\n",
    "\n",
    "# create dataloaders\n",
    "train_args = {'dataset':dataset_train, 'batch_size':512, 'shuffle':True, 'num_workers':8, 'pin_memory':True}\n",
    "dataloader_train = torch.utils.data.DataLoader(**train_args)\n",
    "test_args  = {'dataset':dataset_test, 'batch_size':len(dataset_test), 'shuffle':False, 'num_workers':8}\n",
    "dataloader_test  = torch.utils.data.DataLoader(**test_args) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf0724f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFNN(\n",
      "  (nn): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.4, inplace=False)\n",
      "    (6): Linear(in_features=32, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "using torch on cpu\n",
      "########################################\n",
      "starting training at 2021-07-04 18:18:25\n",
      "########################################\n",
      "epoch:0, total_loss: 0.5397\n",
      "epoch:1, total_loss: 0.5073\n",
      "epoch:2, total_loss: 0.4646\n",
      "epoch:3, total_loss: 0.4162\n",
      "epoch:4, total_loss: 0.3677\n",
      "epoch:5, total_loss: 0.3276\n",
      "epoch:6, total_loss: 0.2990\n",
      "epoch:7, total_loss: 0.2757\n",
      "epoch:8, total_loss: 0.2587\n",
      "epoch:9, total_loss: 0.2451\n",
      "epoch:10, total_loss: 0.2364\n",
      "epoch:11, total_loss: 0.2260\n",
      "epoch:12, total_loss: 0.2170\n",
      "epoch:13, total_loss: 0.2112\n",
      "epoch:14, total_loss: 0.2001\n",
      "epoch:15, total_loss: 0.1961\n",
      "epoch:16, total_loss: 0.1922\n",
      "epoch:17, total_loss: 0.1867\n",
      "epoch:18, total_loss: 0.1828\n"
     ]
    }
   ],
   "source": [
    "spacer = '#' * 40 # for printing purposes\n",
    "\n",
    "# setup model\n",
    "num_classes = 10 # amount of classes in mnist\n",
    "flattened_shape = torch.prod(torch.tensor(dataset_train[0][0].shape))\n",
    "model = FFNN(flattened_shape, num_classes)\n",
    "print(model)\n",
    "\n",
    "# train parameters\n",
    "n_epochs = 20\n",
    "learning_rate = 1e-3\n",
    "\n",
    "optimiser = torch.optim.SGD(params=model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "f_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# configure hardware acceleration\n",
    "device_name = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(device_name)\n",
    "print(f'using torch on {device}')\n",
    "\n",
    "# logging data\n",
    "log = pd.DataFrame(columns=['loss', 'train_accuracy', 'test_accuracy'])\n",
    "\n",
    "# time keeping\n",
    "start_time = time()\n",
    "print(f'{spacer}\\nstarting training at {datetime.fromtimestamp(int(start_time))}\\n{spacer}')\n",
    "\n",
    "### main training loop\n",
    "for n in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    # optimisation\n",
    "    model.train()\n",
    "    for idx, (X, y) in enumerate(dataloader_train):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimiser.zero_grad()\n",
    "        y_pred = model(X) # forward\n",
    "        # y = F.one_hot(y, num_classes) # create one_hot tensor for predictions \n",
    "        # creating one_hot tensor is unnecessary as the torch cross entropy function\n",
    "        # applies softmax and takes argmax from output.\n",
    "        loss = f_loss(y_pred, y)\n",
    "        loss.backward()\n",
    "        total_loss += loss.detach().cpu().item() / len(y) # normalise for batch size\n",
    "        optimiser.step()\n",
    "        \n",
    "    # train set metrics\n",
    "    predictions, targets = [], []\n",
    "    model.eval()\n",
    "    for idx, (X, y) in enumerate(dataloader_train):\n",
    "        predictions.append(model(X).detach())\n",
    "        targets.append(y)\n",
    "        \n",
    "    predictions = torch.cat(predictions, dim=0)\n",
    "    targets = torch.cat(targets, dim=0)\n",
    "    predictions = torch.argmax(F.log_softmax(predictions, dim=1),dim=1)\n",
    "    corrects = (predictions == targets).sum().item()\n",
    "    wrongs = len(targets) - corrects\n",
    "    train_accuracy = corrects / len(targets)\n",
    "    \n",
    "    predictions, targets = [], []\n",
    "    model.eval()\n",
    "    for idx, (X, y) in enumerate(dataloader_test):\n",
    "        predictions.append(model(X).detach())\n",
    "        targets.append(y)\n",
    "        \n",
    "    predictions = torch.cat(predictions, dim=0)\n",
    "    targets = torch.cat(targets, dim=0)\n",
    "    predictions = torch.argmax(F.log_softmax(predictions, dim=1),dim=1)\n",
    "    corrects = (predictions == targets).sum().item()\n",
    "    wrongs = len(targets) - corrects\n",
    "    test_accuracy = corrects / len(targets)\n",
    "        \n",
    "    print(f'epoch:{n}, total_loss: {total_loss:.4f}')\n",
    "    log = log.append({'loss':total_loss, 'train_accuracy':train_accuracy, 'test_accuracy':test_accuracy}, ignore_index=True)\n",
    "\n",
    "# time keeping\n",
    "end_time = time()\n",
    "duration = end_time - start_time\n",
    "print(f'{spacer}\\ntraining finished at {datetime.fromtimestamp(int(end_time))}\\n{spacer}')\n",
    "print(f'\\nscript duration {timedelta(seconds=duration)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830d8777",
   "metadata": {},
   "outputs": [],
   "source": [
    "log.plot(y='loss', title='Cross Entropy Loss over training set', xlabel='epochs', ylabel='loss', legend=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34ee307",
   "metadata": {},
   "outputs": [],
   "source": [
    "log.plot(y=['train_accuracy', 'test_accuracy'], \n",
    "         title='accuracy',\n",
    "         xlabel='epochs', ylabel='correct / total predictions')\n",
    "plt.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
