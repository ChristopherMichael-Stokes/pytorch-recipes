{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56a16226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from time import time\n",
    "from datetime import datetime, timedelta\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter, JupyterNotebookReporter\n",
    "from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd051f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN(nn.Module):\n",
    "    def __init__(self, in_shape, out_shape, p_d1=0.5, p_d2=0.4, h1=64, h2=32):\n",
    "        super().__init__()\n",
    "        fc1 = nn.Linear(in_shape, h1)\n",
    "        a1  = nn.ReLU()\n",
    "        d1  = nn.Dropout(p=p_d1)\n",
    "        fc2 = nn.Linear(h1, h2)\n",
    "        a2  = nn.ReLU()\n",
    "        d2  = nn.Dropout(p=p_d2)\n",
    "        fc3 = nn.Linear(h2, out_shape)\n",
    "        \n",
    "        # not applying log_softmax here, as it is applied later in \n",
    "        # the torch CCE loss\n",
    "        \n",
    "        self.nn = nn.Sequential(fc1, a1, d1, fc2, a2, d2, fc3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.nn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "457496bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mnist(config, epochs, checkpoint_dir=None, data_dir=None):\n",
    "    # create model\n",
    "    model = FFNN(784, 10, \n",
    "                 p_d1=config['p_d1'], \n",
    "                 p_d2=config['p_d2'], \n",
    "                 h1=config['h1'], \n",
    "                 h2=config['h2'])\n",
    "    \n",
    "    # load data and make a validation split\n",
    "    transforms = T.Compose([T.ToTensor(), T.Normalize((0.5,),(0.5)), \n",
    "                            T.Lambda(lambda x: torch.flatten(x))])\n",
    "    dataset_train = MNIST(root='/data/', transform=transforms, train=True)\n",
    "\n",
    "    train_samples = int(len(dataset_train) * 0.8)\n",
    "    train_subset, val_subset = random_split(dataset_train,\n",
    "                                           [train_samples, \n",
    "                                            len(dataset_train) - train_samples])\n",
    "    # create dataloaders\n",
    "    train_args = {'dataset':train_subset, \n",
    "                  'batch_size':config['batch_size'], \n",
    "                  'shuffle':True, \n",
    "                  'num_workers':8, \n",
    "                  'pin_memory':True}\n",
    "    dataloader_train = torch.utils.data.DataLoader(**train_args)\n",
    "    val_args  = {'dataset':val_subset, \n",
    "                  'batch_size':len(val_subset), \n",
    "                  'shuffle':False, \n",
    "                  'num_workers':8}\n",
    "    dataloader_val  = torch.utils.data.DataLoader(**val_args) \n",
    "    \n",
    "    # choose computation host device\n",
    "    device_name = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    device = torch.device(device_name)\n",
    "    model.to(device)\n",
    "    \n",
    "    \n",
    "    optimiser = torch.optim.SGD(params=model.parameters(), lr=config['lr'], momentum=0.9)\n",
    "    f_loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # training loop\n",
    "    for n in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        # optimisation\n",
    "        model.train()\n",
    "        for idx, (X, y) in enumerate(dataloader_train):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimiser.zero_grad()\n",
    "            y_pred = model(X)\n",
    "            loss = f_loss(y_pred, y)\n",
    "            loss.backward()\n",
    "            total_loss += loss.detach().cpu().item() / len(y) # normalise for batch size\n",
    "            optimiser.step()\n",
    "            \n",
    "        # validation set metrics\n",
    "        predictions, targets, val_losses = [], [], []\n",
    "        model.eval()\n",
    "        # we are adding the metrics tensor for each batch to a list,\n",
    "        # then concatenating at the end to make one tensor with all samples\n",
    "        for idx, (X, y) in enumerate(dataloader_val):\n",
    "            with torch.no_grad():\n",
    "                y_pred = model(X)\n",
    "                predictions.append(y_pred.detach())\n",
    "                targets.append(y)\n",
    "                val_losses.append(f_loss(y_pred, y).cpu().item())\n",
    "\n",
    "        predictions = torch.cat(predictions, dim=0)\n",
    "        targets = torch.cat(targets, dim=0)\n",
    "        predictions = torch.argmax(F.log_softmax(predictions, dim=1),dim=1)\n",
    "        corrects = (predictions == targets).sum().item()\n",
    "        wrongs = len(targets) - corrects\n",
    "        val_accuracy = corrects / len(targets)\n",
    "        val_loss = sum(val_losses) / float(len(val_losses))\n",
    "        \n",
    "        # save checkpoint\n",
    "        with tune.checkpoint_dir(n) as checkpoint_dir:\n",
    "            path = osp.join(checkpoint_dir, 'checkpoint')\n",
    "            torch.save((model.state_dict(), optimiser.state_dict()), path)\n",
    "            \n",
    "        # report metric values back to main scheduler\n",
    "        tune.report(loss=val_loss, accuracy=val_accuracy)\n",
    "        \n",
    "def test_accuracy(model, device='cpu'):\n",
    "    transforms = T.Compose([T.ToTensor(), T.Normalize((0.5,),(0.5)), \n",
    "                            T.Lambda(lambda x: torch.flatten(x))])\n",
    "    dataset_test  = MNIST(root='/data/', transform=transforms, train=False)\n",
    "    test_args  = {'dataset':dataset_test, \n",
    "                  'batch_size':len(dataset_test), \n",
    "                  'shuffle':False, \n",
    "                  'num_workers':8}\n",
    "    dataloader_test  = torch.utils.data.DataLoader(**test_args) \n",
    "    \n",
    "    model.eval()\n",
    "    predictions, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for (X, y) in dataloader_test:\n",
    "            y_pred = model(X)\n",
    "            predictions.append(y_pred.detach())\n",
    "            targets.append(y)\n",
    "\n",
    "        predictions = torch.cat(predictions, dim=0)\n",
    "        targets = torch.cat(targets, dim=0)\n",
    "        predictions = torch.argmax(F.log_softmax(predictions, dim=1),dim=1)\n",
    "        corrects = (predictions == targets).sum().item()\n",
    "        wrongs = len(targets) - corrects\n",
    "        test_accuracy = corrects / len(targets)\n",
    "        \n",
    "    return  test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "494aacb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main(max_epochs=20, num_trials=30, is_notebook=True):\n",
    "    config = {'lr':tune.loguniform(1e-3, 1e-1), \n",
    "              'batch_size':tune.choice([32, 64, 128, 256, 512]), \n",
    "              'p_d1':tune.uniform(0.2, 0.9), \n",
    "              'p_d2':tune.uniform(0.2, 0.9), \n",
    "              'h1':tune.choice([64, 256, 512, 1024]), \n",
    "              'h2':tune.choice([32, 64, 256, 512])}\n",
    "\n",
    "\n",
    "    scheduler = ASHAScheduler(metric='loss', \n",
    "                            mode='min',\n",
    "                            max_t=max_epochs, \n",
    "                            grace_period=2, \n",
    "                            reduction_factor=2)\n",
    "    \n",
    "    metric_columns = ['loss', 'accuracy', 'training_iteration']\n",
    "    if is_notebook:\n",
    "        reporter = JupyterNotebookReporter(overwrite=True, max_progress_rows=num_trials, metric_columns=metric_columns)\n",
    "    else:\n",
    "        reporter = CLIReporter(metric_columns=metric_columns)\n",
    "    #reporter  = CLIReporter(metric_columns=['loss', 'accuracy', 'training_iteration'])\n",
    "\n",
    "    resources = {'cpu':2} \n",
    "    if torch.cuda.is_available():\n",
    "        resources['gpu'] = 0.5\n",
    "\n",
    "    result = tune.run(partial(train_mnist, epochs=max_epochs),\n",
    "                      resources_per_trial=resources, \n",
    "                      config=config, \n",
    "                      num_samples=num_trials, \n",
    "                      scheduler=scheduler, \n",
    "                      progress_reporter=reporter)\n",
    "\n",
    "    best_trial = result.get_best_trial('loss', 'min', 'last')\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "            best_trial.last_result[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(\n",
    "            best_trial.last_result[\"accuracy\"]))\n",
    "    best_trained_model = FFNN(784, 10, \n",
    "                              p_d1=best_trial.config['p_d1'], \n",
    "                              p_d2=best_trial.config['p_d2'], \n",
    "                              h1=best_trial.config['h1'], \n",
    "                              h2=best_trial.config['h2'])\n",
    "\n",
    "    best_checkpoint_dir = best_trial.checkpoint.value\n",
    "    model_state, optimiser_state = torch.load(osp.join(best_checkpoint_dir, \"checkpoint\"))\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "    test_acc = test_accuracy(best_trained_model)\n",
    "    print(\"Best trial test set accuracy: {}\".format(test_acc))\n",
    "    \n",
    "    return best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9f2dc53",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 1.8/7.7 GiB<br>Using AsyncHyperBand: num_stopped=30\n",
       "Bracket: Iter 16.000: -0.11276781558990479 | Iter 8.000: -0.18138695508241653 | Iter 4.000: -0.2486966997385025 | Iter 2.000: -0.40189415216445923<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/3.95 GiB heap, 0.0/1.97 GiB objects (0.0/2.0 CPU_group_1bc56fcf46cc705d2563b63c2c1b4f21, 0.0/2.0 CPU_group_0_1bc56fcf46cc705d2563b63c2c1b4f21)<br>Result logdir: /home/chris/ray_results/DEFAULT_2021-07-07_14-08-46<br>Number of trials: 30/30 (30 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  h1</th><th style=\"text-align: right;\">  h2</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">    p_d1</th><th style=\"text-align: right;\">    p_d2</th><th style=\"text-align: right;\">     loss</th><th style=\"text-align: right;\">  accuracy</th><th style=\"text-align: right;\">  training_iteration</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DEFAULT_764c3_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">0.00113287</td><td style=\"text-align: right;\">0.359588</td><td style=\"text-align: right;\">0.493409</td><td style=\"text-align: right;\">0.465769 </td><td style=\"text-align: right;\">  0.866583</td><td style=\"text-align: right;\">                   2</td></tr>\n",
       "<tr><td>DEFAULT_764c3_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0228883 </td><td style=\"text-align: right;\">0.815   </td><td style=\"text-align: right;\">0.552445</td><td style=\"text-align: right;\">0.189169 </td><td style=\"text-align: right;\">  0.949083</td><td style=\"text-align: right;\">                  20</td></tr>\n",
       "<tr><td>DEFAULT_764c3_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0351322 </td><td style=\"text-align: right;\">0.564819</td><td style=\"text-align: right;\">0.800011</td><td style=\"text-align: right;\">0.390777 </td><td style=\"text-align: right;\">  0.909167</td><td style=\"text-align: right;\">                  20</td></tr>\n",
       "<tr><td>DEFAULT_764c3_00003</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0363556 </td><td style=\"text-align: right;\">0.830897</td><td style=\"text-align: right;\">0.672587</td><td style=\"text-align: right;\">2.30279  </td><td style=\"text-align: right;\">  0.107917</td><td style=\"text-align: right;\">                   2</td></tr>\n",
       "<tr><td>DEFAULT_764c3_00004</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.00358305</td><td style=\"text-align: right;\">0.44288 </td><td style=\"text-align: right;\">0.781101</td><td style=\"text-align: right;\">0.143297 </td><td style=\"text-align: right;\">  0.961583</td><td style=\"text-align: right;\">                  20</td></tr>\n",
       "<tr><td>DEFAULT_764c3_00005</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">         512</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">0.0129348 </td><td style=\"text-align: right;\">0.430292</td><td style=\"text-align: right;\">0.894693</td><td style=\"text-align: right;\">0.112558 </td><td style=\"text-align: right;\">  0.965833</td><td style=\"text-align: right;\">                  20</td></tr>\n",
       "<tr><td>DEFAULT_764c3_00006</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.020376  </td><td style=\"text-align: right;\">0.396604</td><td style=\"text-align: right;\">0.48484 </td><td style=\"text-align: right;\">0.22516  </td><td style=\"text-align: right;\">  0.941   </td><td style=\"text-align: right;\">                   8</td></tr>\n",
       "<tr><td>DEFAULT_764c3_00007</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.00320777</td><td style=\"text-align: right;\">0.541586</td><td style=\"text-align: right;\">0.257281</td><td style=\"text-align: right;\">0.428592 </td><td style=\"text-align: right;\">  0.8805  </td><td style=\"text-align: right;\">                   2</td></tr>\n",
       "<tr><td>DEFAULT_764c3_00008</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0833813 </td><td style=\"text-align: right;\">0.270767</td><td style=\"text-align: right;\">0.43672 </td><td style=\"text-align: right;\">0.0830999</td><td style=\"text-align: right;\">  0.973917</td><td style=\"text-align: right;\">                  20</td></tr>\n",
       "<tr><td>DEFAULT_764c3_00009</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.00577403</td><td style=\"text-align: right;\">0.396654</td><td style=\"text-align: right;\">0.48296 </td><td style=\"text-align: right;\">0.0948996</td><td style=\"text-align: right;\">  0.972333</td><td style=\"text-align: right;\">                  20</td></tr>\n",
       "<tr><td>DEFAULT_764c3_00010</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0035407 </td><td style=\"text-align: right;\">0.426879</td><td style=\"text-align: right;\">0.538364</td><td style=\"text-align: right;\">0.423414 </td><td style=\"text-align: right;\">  0.8785  </td><td style=\"text-align: right;\">                   2</td></tr>\n",
       "<tr><td>DEFAULT_764c3_00011</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">0.032075  </td><td style=\"text-align: right;\">0.275375</td><td style=\"text-align: right;\">0.518629</td><td style=\"text-align: right;\">0.0748799</td><td style=\"text-align: right;\">  0.981   </td><td style=\"text-align: right;\">                  20</td></tr>\n",
       "<tr><td>DEFAULT_764c3_00012</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">         512</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0485765 </td><td style=\"text-align: right;\">0.897047</td><td style=\"text-align: right;\">0.302769</td><td style=\"text-align: right;\">1.6426   </td><td style=\"text-align: right;\">  0.533333</td><td style=\"text-align: right;\">                   2</td></tr>\n",
       "<tr><td>DEFAULT_764c3_00013</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0108363 </td><td style=\"text-align: right;\">0.897898</td><td style=\"text-align: right;\">0.744283</td><td style=\"text-align: right;\">1.25707  </td><td style=\"text-align: right;\">  0.607583</td><td style=\"text-align: right;\">                   2</td></tr>\n",
       "<tr><td>DEFAULT_764c3_00014</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.0573464 </td><td style=\"text-align: right;\">0.713218</td><td style=\"text-align: right;\">0.570538</td><td style=\"text-align: right;\">0.326407 </td><td style=\"text-align: right;\">  0.909   </td><td style=\"text-align: right;\">                   4</td></tr>\n",
       "<tr><td>DEFAULT_764c3_00015</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0421822 </td><td style=\"text-align: right;\">0.581119</td><td style=\"text-align: right;\">0.444326</td><td style=\"text-align: right;\">0.293216 </td><td style=\"text-align: right;\">  0.91475 </td><td style=\"text-align: right;\">                   4</td></tr>\n",
       "<tr><td>DEFAULT_764c3_00016</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">         512</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.086484  </td><td style=\"text-align: right;\">0.754366</td><td style=\"text-align: right;\">0.25534 </td><td style=\"text-align: right;\">0.240968 </td><td style=\"text-align: right;\">  0.932583</td><td style=\"text-align: right;\">                   8</td></tr>\n",
       "<tr><td>DEFAULT_764c3_00017</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.0221934 </td><td style=\"text-align: right;\">0.783402</td><td style=\"text-align: right;\">0.345364</td><td style=\"text-align: right;\">0.594376 </td><td style=\"text-align: right;\">  0.872   </td><td style=\"text-align: right;\">                   2</td></tr>\n",
       "<tr><td>DEFAULT_764c3_00018</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.00394398</td><td style=\"text-align: right;\">0.529415</td><td style=\"text-align: right;\">0.516402</td><td style=\"text-align: right;\">0.380374 </td><td style=\"text-align: right;\">  0.89975 </td><td style=\"text-align: right;\">                   2</td></tr>\n",
       "<tr><td>DEFAULT_764c3_00019</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0104827 </td><td style=\"text-align: right;\">0.672656</td><td style=\"text-align: right;\">0.88123 </td><td style=\"text-align: right;\">0.44391  </td><td style=\"text-align: right;\">  0.883583</td><td style=\"text-align: right;\">                   2</td></tr>\n",
       "<tr><td>DEFAULT_764c3_00020</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">0.0375052 </td><td style=\"text-align: right;\">0.364608</td><td style=\"text-align: right;\">0.279777</td><td style=\"text-align: right;\">0.102946 </td><td style=\"text-align: right;\">  0.973667</td><td style=\"text-align: right;\">                  20</td></tr>\n",
       "<tr><td>DEFAULT_764c3_00021</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">0.00156686</td><td style=\"text-align: right;\">0.249856</td><td style=\"text-align: right;\">0.804425</td><td style=\"text-align: right;\">0.428694 </td><td style=\"text-align: right;\">  0.883333</td><td style=\"text-align: right;\">                   2</td></tr>\n",
       "<tr><td>DEFAULT_764c3_00022</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0277538 </td><td style=\"text-align: right;\">0.38232 </td><td style=\"text-align: right;\">0.636851</td><td style=\"text-align: right;\">0.195667 </td><td style=\"text-align: right;\">  0.947417</td><td style=\"text-align: right;\">                   8</td></tr>\n",
       "<tr><td>DEFAULT_764c3_00023</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">0.0140534 </td><td style=\"text-align: right;\">0.818015</td><td style=\"text-align: right;\">0.371125</td><td style=\"text-align: right;\">1.13636  </td><td style=\"text-align: right;\">  0.682   </td><td style=\"text-align: right;\">                   2</td></tr>\n",
       "<tr><td>DEFAULT_764c3_00024</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0124138 </td><td style=\"text-align: right;\">0.605158</td><td style=\"text-align: right;\">0.453231</td><td style=\"text-align: right;\">0.306956 </td><td style=\"text-align: right;\">  0.915083</td><td style=\"text-align: right;\">                   4</td></tr>\n",
       "<tr><td>DEFAULT_764c3_00025</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.00124228</td><td style=\"text-align: right;\">0.336353</td><td style=\"text-align: right;\">0.696401</td><td style=\"text-align: right;\">0.693702 </td><td style=\"text-align: right;\">  0.83775 </td><td style=\"text-align: right;\">                   2</td></tr>\n",
       "<tr><td>DEFAULT_764c3_00026</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">0.0234324 </td><td style=\"text-align: right;\">0.454172</td><td style=\"text-align: right;\">0.547757</td><td style=\"text-align: right;\">0.52944  </td><td style=\"text-align: right;\">  0.86425 </td><td style=\"text-align: right;\">                   2</td></tr>\n",
       "<tr><td>DEFAULT_764c3_00027</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0134677 </td><td style=\"text-align: right;\">0.733689</td><td style=\"text-align: right;\">0.41024 </td><td style=\"text-align: right;\">0.102513 </td><td style=\"text-align: right;\">  0.970833</td><td style=\"text-align: right;\">                  20</td></tr>\n",
       "<tr><td>DEFAULT_764c3_00028</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.00123506</td><td style=\"text-align: right;\">0.294429</td><td style=\"text-align: right;\">0.460291</td><td style=\"text-align: right;\">0.584886 </td><td style=\"text-align: right;\">  0.852167</td><td style=\"text-align: right;\">                   2</td></tr>\n",
       "<tr><td>DEFAULT_764c3_00029</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">0.0562018 </td><td style=\"text-align: right;\">0.290174</td><td style=\"text-align: right;\">0.529118</td><td style=\"text-align: right;\">2.30268  </td><td style=\"text-align: right;\">  0.113583</td><td style=\"text-align: right;\">                   2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-07 14:27:03,723\tINFO tune.py:549 -- Total run time: 1097.59 seconds (1097.41 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial config: {'lr': 0.03207500554634961, 'batch_size': 128, 'p_d1': 0.27537547962514997, 'p_d2': 0.5186288256994812, 'h1': 1024, 'h2': 512}\n",
      "Best trial final validation loss: 0.07487986981868744\n",
      "Best trial final validation accuracy: 0.981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/.local/lib/python3.9/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:174.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial test set accuracy: 0.9803\n"
     ]
    }
   ],
   "source": [
    "best_trial = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a3115be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial config: {'lr': 0.03207500554634961, 'batch_size': 128, 'p_d1': 0.27537547962514997, 'p_d2': 0.5186288256994812, 'h1': 1024, 'h2': 512}\n",
      "Best trial final validation loss: 0.07487986981868744\n",
      "Best trial final validation accuracy: 0.981\n"
     ]
    }
   ],
   "source": [
    "print(\"Best trial config: {}\".format(best_trial.config))\n",
    "print(\"Best trial final validation loss: {}\".format(\n",
    "        best_trial.last_result[\"loss\"]))\n",
    "print(\"Best trial final validation accuracy: {}\".format(\n",
    "        best_trial.last_result[\"accuracy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f23cce40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial test set accuracy: 0.9803\n"
     ]
    }
   ],
   "source": [
    "best_trained_model = FFNN(784, 10, \n",
    "                          p_d1=best_trial.config['p_d1'], \n",
    "                          p_d2=best_trial.config['p_d2'], \n",
    "                          h1=best_trial.config['h1'], \n",
    "                          h2=best_trial.config['h2'])\n",
    "\n",
    "best_checkpoint_dir = best_trial.checkpoint.value\n",
    "model_state, optimiser_state = torch.load(osp.join(best_checkpoint_dir, \"checkpoint\"))\n",
    "best_trained_model.load_state_dict(model_state)\n",
    "test_acc = test_accuracy(best_trained_model)\n",
    "print(\"Best trial test set accuracy: {}\".format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
